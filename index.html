<html lang="en-US">
  <head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

<!-- Begin Jekyll SEO tag v2.7.1 -->
<title>Neural Text to Speech Synthesis | Tutorial @ IJCAI 2021</title>
<meta name="generator" content="Jekyll v3.9.0" />
<meta property="og:title" content="Neural Text to Speech Synthesis" />
<meta property="og:locale" content="en_US" />
<link rel="canonical" href="https://tts-tutorial.github.io/ijcai2021/" />
<meta property="og:url" content="https://tts-tutorial.github.io/ijcai2021/" />
<meta property="og:site_name" content="ijcai2021" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Neural Text to Speech Synthesis" />
<script type="application/ld+json">
{"headline":"Neural Text to Speech Synthesis","url":"https://tts-tutorial.github.io/ijcai2021/","@type":"WebSite","name":"ijcai2021","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

  </head>
  <body>
    <div class="container-lg px-3 my-5 markdown-body">
      
      <h1 id="Non-Autoregressive-Sequence-Generation">Non-Autoregressive Sequence Generation</h1>
<p>Tutorial @ <a href="https://www.2022.aclweb.org/">ACL 2022</a>, May 22-27, 2022</p>

<h2 id="speakers">Speakers</h2>
<p>
  <a href="https://jiataogu.me/">Jiatao Gu</a>, Facebook AI Research, <a href="mailto:jgu@fb.com">jgu@fb.com</a>
  <br />
  <a href="https://www.microsoft.com/en-us/research/people/xuta/">Xu Tan</a>, Microsoft Research Asia, <a href="mailto:xuta@microsoft.com">xuta@microsoft.com</a> 
</p>

<h2 id="abstract">Abstract</h2>
<p>Non-autoregressive sequence generation (NAR) attempts to generate the entire or partial output sequences in parallel to speed up the generation process and avoid potential issues (e.g., label bias, exposure bias) in autoregressive generation. While it has received much research attention and has been applied in many sequence generation tasks in natural language and speech, naive NAR models still face many challenges to close the performance gap between state-of-the-art autoregressive models because of a lack of modeling power. In this tutorial, we will provide a thorough introduction and review of non-autoregressive sequence generation, in four sections: 1) Background, which covers the motivation of NAR generation, the problem definition, the evaluation protocol, and the comparison with standard autoregressive generation approaches. 2) Method, which includes different aspects: model architecture, objective function, training data, learning paradigm, and additional inference tricks. 3) Application, which covers different tasks in text and speech generation, and some advanced topics in applications. 4) Conclusion, in which we describe several research challenges and discuss the potential future research directions. We hope this tutorial can serve both academic researchers and industry practitioners working on non-autoregressive sequence generation. </p>

      
<h2 id="outline">Outline</h2> 
<ol>    
  <li>PART 1  Introduction (~ 20 minutes)<br />
          1.1 Problem definition<br /> 
          1.2 Evaluation protocol<br /> 
          1.3 Multi-modality problem<br /></li>  
  <li>PART 2  Methods  (~ 90 minutes)<br /> 
          2.1 Model architectures<br /> 
            2.1.1  Fully NAR models<br /> 
            2.1.2  Iteration-based NAR models<br /> 
            2.1.3  Partially NAR models<br /> 
            2.1.4  Locally AR models<br /> 
            2.1.5  NAR models with latent variables<br /> 
          2.2 Objective functions<br /> 
            2.2.1  Loss with latent variables<br /> 
            2.2.2  Loss beyond token-level<br /> 
          2.3 Training data<br /> 
          2.4 Learning paradigms<br /> 
            2.4.1  Curriculum learning<br /> 
            2.4.2  Adversarial training<br /> 
            2.4.3  Self-supervised pre-training<br /> 
          2.5 Inference methods and tricks<br /></li> 
  <li>PART 3  Applications  (~ 50 minutes)<br /> 
          3.1 Text generation<br /> 
            3.1.1  Neural machine translation<br /> 
            3.1.2  Text summarization<br /> 
            3.1.3  Text error correction<br /> 
            3.1.4  Automatic speech recognition<br /> 
          3.2 Speech generation<br /> 
            3.2.1  Text to speech<br /> 
            3.2.2  Voice conversion<br /> 
          3.3 Advanced topics in applications<br /> 
            3.3.1   Advanced length prediction<br /> 
            3.3.2  Alignment (duration vs attention)<br />
            3.3.3  Target token dependency<br />
            3.3.4  Relationship with streaming<br /></li>           
  <li>PART 4  Open problems, future directions, Q\&A  (~ 20 minutes)<br /></li>
</ol>       
      
      

<h2 id="materials">Materials</h2>
<p><a href="tbd">Slides (To be released)</a><br />



      
    </div>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/anchor-js/4.1.0/anchor.min.js" integrity="sha256-lZaRhKri35AyJSypXXs4o6OPFTbTmUoltBbDCbdzegg=" crossorigin="anonymous"></script>
    <script>anchors.add();</script>
    
  </body>
</html>


